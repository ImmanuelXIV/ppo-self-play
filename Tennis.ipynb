{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning | Multi-Agent RL | Self-Play | Proximal Policy Optimization Algorithm (PPO) agent | Unity Tennis environment | Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This repository, shows how to implement and train an actor-critic [PPO](https://arxiv.org/abs/1707.06347) (Proximal Policy Optimization) Reinforcement Learning agent to play Tennis against itself. The Unity simulation environment is called Tennis and rather similar to environments depicted [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md).\n",
    "\n",
    "In this README.md you'll see how to install dependencies and run the code on your own machine. To understand the learning algorithm PPO checkout the `Tennis.ipynb` notebook.\n",
    "\n",
    "**Why?** Reinforcement Learning (RL) is one of the most fascinating areas of Machine Learning! It is quite intuitive, because we use positive and negative feedback to learn tasks via interaction with the environment. The PPO algorithm, by Schulman et al. 2017, has been used at OpenAi to solve complex real-world tasks such as manipulating physical objects with a robot hand. Check out this [Learning Dexterity: Uncut](https://www.youtube.com/watch?time_continue=1&v=DKe8FumoD4E&feature=emb_logo) video, or the ones about simulated humanoid robots from their website [here](https://openai.com/blog/openai-baselines-ppo/) to get an idea! \n",
    "\n",
    "**What?** In this Tennis environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play. This is a trained agent in the environment.\n",
    "\n",
    "<img src=\"imgs/tennis-self-play.gif\\\" width=\"450\" align=\"center\" title=\"Tennis Unity environment\">\n",
    "\n",
    "**How?** Checkout the `Tennis.ipynb` notebook to learn more about the PPO algorithm, and check the implementations in `ppo_agent.py`, `policy.py`. If you want to train an agent, or see a trained agent play tennis then `train.py`, and `watch_trained_agent.py` are the go-to files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Learning Algorithm\n",
    "\n",
    "The Proximal Policy Optimization [PPO](https://arxiv.org/abs/1707.06347) algorithm was developed by Schulmann et al. in 2017 at the company [OpenAI](https://openai.com/). The abstract reads as follows: \n",
    ">We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.\n",
    "\n",
    "This is the PPo algorithm (taken from [source](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf)). \n",
    "\n",
    "<img src=\"imgs/PPO-algorithm.png\" width=\"600\" align=\"center\" title=\"PPO algorithm\"/>\n",
    "\n",
    "The PPO computes a surrogate loss function (see above) without computing natural gradients and enables us to re-use previously collected trajectories from an older version of the same policy. For details please refer to the paper. In this notebook we use the following hyperparameters and model architecture. Refer to the code for more details.\n",
    "\n",
    "- SEED = 123\n",
    "- SEED = 0\n",
    "- LR = 5e-4\n",
    "- T_MAX_ROLLOUT = 1024\n",
    "- GAMMA = 0.999\n",
    "- TAU = 0.95\n",
    "- K_EPOCHS = 16\n",
    "- BATCH_SIZE = 64\n",
    "- EPSILON_PPO = 0.2\n",
    "- USE_ENTROPY = False\n",
    "- ENTROPY_WEIGHT = 0.01\n",
    "- GRADIENT_CLIPPING = 2 \n",
    "\n",
    "The chosen actor critic model architecture is as follows. The actor and crtic network share the same fully connected body and have 5957 free, trainable parameters.\n",
    "\n",
    "```python\n",
    "ActorCritic(\n",
    "  (fc1_body): Linear(in_features=24, out_features=64, bias=True)\n",
    "  (fc2_body): Linear(in_features=64, out_features=64, bias=True)\n",
    "  (fc3_actor): Linear(in_features=64, out_features=2, bias=True)\n",
    "  (fc3_critic): Linear(in_features=64, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(1, 6):                                      # play game for 5 episodes\\n    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \\n    states = env_info.vector_observations                  # get the current state (for each agent)\\n    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\\n    t = 0\\n    \\n    while True:\\n        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\\n        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\\n        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\\n        next_states = env_info.vector_observations         # get next state (for each agent)\\n        rewards = env_info.rewards                         # get reward (for each agent)\\n        dones = env_info.local_done                        # see if episode finished\\n        scores += env_info.rewards                         # update the score (for each agent)\\n        states = next_states                               # roll over states to next time step\\n        t += 1\\n        if np.any(dones):                                  # exit loop if episode finished\\n            print('Espisode finished after {} time steps.'.format(t))\\n            break\\n    print('Score (max over agents) from episode {}: {:.3f}'.format(i, np.max(scores)))\\nenv.close()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    t = 0\n",
    "    \n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        t += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            print('Espisode finished after {} time steps.'.format(t))\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {:.3f}'.format(i, np.max(scores)))\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a PPO Agent\n",
    "\n",
    "I'd recommend that you open the files `ppo_agent.py`, and `policy.py` in different tabs and walk through the code step by step to gain a deeper understanding. Training a PPO agent from scratch to solving the environment took approximately 02:00h on a 2.7 GHz Intel Core i5 (2015). Restart the kernel and only run the code underneath. If you just want to see a trained agent playing tennis, then jump to section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Number of trainable actor critic model parameters:  5957\n",
      "10/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0390\n",
      "20/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0295\n",
      "30/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0260\n",
      "40/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0293\n",
      "50/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0254\n",
      "60/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0258\n",
      "70/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0290\n",
      "80/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0339\n",
      "90/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0344\n",
      "100/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0358\n",
      "110/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0358\n",
      "120/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0408\n",
      "130/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0447\n",
      "140/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0447\n",
      "150/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0487\n",
      "160/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0489\n",
      "170/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0491\n",
      "180/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0471\n",
      "190/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0461\n",
      "200/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0442\n",
      "210/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0442\n",
      "220/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0410\n",
      "230/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0381\n",
      "240/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0408\n",
      "250/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0398\n",
      "260/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0396\n",
      "270/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0394\n",
      "280/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0415\n",
      "290/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0416\n",
      "300/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0447\n",
      "310/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0437\n",
      "320/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0478\n",
      "330/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0459\n",
      "340/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0433\n",
      "350/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0461\n",
      "360/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0471\n",
      "370/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0502\n",
      "380/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0482\n",
      "390/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0492\n",
      "400/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0480\n",
      "410/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0490\n",
      "420/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0450\n",
      "430/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0509\n",
      "440/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0508\n",
      "450/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0499\n",
      "460/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0481\n",
      "470/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0431\n",
      "480/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0420\n",
      "490/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0438\n",
      "500/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0420\n",
      "510/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0410\n",
      "520/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0429\n",
      "530/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0419\n",
      "540/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0428\n",
      "550/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0409\n",
      "560/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0437\n",
      "570/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0467\n",
      "580/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0509\n",
      "590/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0511\n",
      "600/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0491\n",
      "610/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0531\n",
      "620/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.0542\n",
      "630/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0551\n",
      "640/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0563\n",
      "650/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0570\n",
      "660/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0561\n",
      "670/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0551\n",
      "680/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0548\n",
      "690/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0567\n",
      "700/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0617\n",
      "710/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0618\n",
      "720/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0607\n",
      "730/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0598\n",
      "740/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0587\n",
      "750/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0590\n",
      "760/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0591\n",
      "770/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0602\n",
      "780/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0614\n",
      "790/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0575\n",
      "800/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0573\n",
      "810/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.0560\n",
      "820/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0582\n",
      "830/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.0629\n",
      "840/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0629\n",
      "850/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0649\n",
      "860/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0658\n",
      "870/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0648\n",
      "880/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0628\n",
      "890/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0658\n",
      "900/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.0679\n",
      "910/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0650\n",
      "920/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0638\n",
      "930/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0581\n",
      "940/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0572\n",
      "950/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0572\n",
      "960/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0642\n",
      "970/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.0701\n",
      "980/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0732\n",
      "990/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0792\n",
      "1000/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0783\n",
      "1010/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.0854\n",
      "1020/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0886\n",
      "1030/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.0957\n",
      "1040/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.0986\n",
      "1050/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1036\n",
      "1060/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.1077\n",
      "1070/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.1097\n",
      "1080/2000 Episode. Current score: 0.5000 Avg last 100 score: 0.1137\n",
      "1090/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.1137\n",
      "1100/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1236\n",
      "1110/2000 Episode. Current score: 0.3900 Avg last 100 score: 0.1336\n",
      "1120/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.1446\n",
      "1130/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1566\n",
      "1140/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1677\n",
      "1150/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1767\n",
      "1160/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.1846\n",
      "1170/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1866\n",
      "1180/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.1866\n",
      "1190/2000 Episode. Current score: 0.9000 Avg last 100 score: 0.1944\n",
      "1200/2000 Episode. Current score: 0.3000 Avg last 100 score: 0.1974\n",
      "1210/2000 Episode. Current score: 0.8000 Avg last 100 score: 0.2244\n",
      "1220/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2334\n",
      "1230/2000 Episode. Current score: 0.7000 Avg last 100 score: 0.2314\n",
      "1240/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.2343\n",
      "1250/2000 Episode. Current score: 0.4000 Avg last 100 score: 0.2332\n",
      "1260/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.2222\n",
      "1270/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2252\n",
      "1280/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2242\n",
      "1290/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.2224\n",
      "1300/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2285\n",
      "1310/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2025\n",
      "1320/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.1955\n",
      "1330/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2014\n",
      "1340/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.2135\n",
      "1350/2000 Episode. Current score: 0.8000 Avg last 100 score: 0.2405\n",
      "1360/2000 Episode. Current score: 0.4000 Avg last 100 score: 0.2625\n",
      "1370/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.2736\n",
      "1380/2000 Episode. Current score: 0.0000 Avg last 100 score: 0.2955\n",
      "1390/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.3285\n",
      "1400/2000 Episode. Current score: 0.6900 Avg last 100 score: 0.3303\n",
      "1410/2000 Episode. Current score: 0.9000 Avg last 100 score: 0.3493\n",
      "1420/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.3452\n",
      "1430/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.3542\n",
      "1440/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.3511\n",
      "1450/2000 Episode. Current score: 0.3000 Avg last 100 score: 0.3490\n",
      "1460/2000 Episode. Current score: 1.2000 Avg last 100 score: 0.3411\n",
      "1470/2000 Episode. Current score: 0.1000 Avg last 100 score: 0.3371\n",
      "1480/2000 Episode. Current score: 0.4000 Avg last 100 score: 0.3421\n",
      "1490/2000 Episode. Current score: 0.2000 Avg last 100 score: 0.3651\n",
      "1500/2000 Episode. Current score: 0.5000 Avg last 100 score: 0.3953\n",
      "1510/2000 Episode. Current score: 0.0900 Avg last 100 score: 0.3963\n",
      "1520/2000 Episode. Current score: 0.4000 Avg last 100 score: 0.4744\n",
      "Environment solved after 1524 episodes. Avg last 100 score: 0.51655\n",
      "\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from ppo_agent import Agent\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# start unity environment\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "number_of_agents = len(env_info.agents)\n",
    "\n",
    "print_every = 10\n",
    "\n",
    "\n",
    "\n",
    "def run_ppo(env, brain_name, agent, num_episodes=2000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        agent.step(env, brain_name)\n",
    "        max_score = agent.act(env, brain_name)\n",
    "        scores.append(max_score)\n",
    "        scores_window.append(max_score)\n",
    "\n",
    "        print('\\r{}/{} Episode. Current score: {:.4f} Avg last 100 score: {:.4f}'.\\\n",
    "            format(i_episode, num_episodes, max_score, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\r{}/{} Episode. Current score: {:.4f} Avg last 100 score: {:.4f}'.\\\n",
    "                format(i_episode, num_episodes, max_score, np.mean(scores_window)))\n",
    "\n",
    "        if np.mean(scores_window) > 0.5:\n",
    "            agent.save()\n",
    "            print('\\rEnvironment solved after {} episodes. Avg last 100 score: {:.4f}'.\\\n",
    "                format(i_episode, np.mean(scores_window)))\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"Start Training...\")\n",
    "agent = Agent(state_size, action_size, load_pretrained=False)\n",
    "scores = run_ppo(env, brain_name, agent)\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time a score >= 0.5 was reached at episode 1079.\n",
      "Max score reached: 2.7000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scores = np.array(scores)\n",
    "x = np.where(scores >= 0.5)\n",
    "print('The first time a score >= 0.5 was reached at episode {}.'.format(x[0][0]))\n",
    "print('Max score reached: {:.4f}'.format(np.amax(scores)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': np.arange(len(scores)),\n",
    "    'y': scores, \n",
    "    })\n",
    "rolling_mean = df.y.rolling(window=50).mean()\n",
    "\n",
    "#img_path =\"imgs/scores_plot.png\"\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(df.x, df.y, label='Scores')\n",
    "plt.plot(df.x, rolling_mean, label='Moving avg', color='orange')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#fig.savefig(fname=img_path)\n",
    "#print('\\nPlot saved to {}.'.format(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch a trained Agent\n",
    "\n",
    "Restart the kernel and run the code cell below to load a trained agent and run it in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable actor critic model parameters:  5957\n",
      "Loading pre-trained actor critic model from checkpoint.\n",
      "Episode 1 finished. Scores reached: [0.70000001 0.69000001]\n",
      "Episode 2 finished. Scores reached: [1.49000002 1.60000002]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "from ppo_agent import Agent\n",
    "\n",
    "\n",
    "# start unity environment\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "agent = Agent(state_size, action_size, load_pretrained=True)\n",
    "num_episodes = 2\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    states1 = states[0]\n",
    "    states2 = states[1]\n",
    "    scores = np.zeros(2)\n",
    "    agent.ac_model.eval()\n",
    "\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            # self-play: same actor critic model is used for two players\n",
    "            actions1, _, _, _ = agent.ac_model(states1)\n",
    "            actions2, _, _, _ = agent.ac_model(states2)\n",
    "        actions = torch.cat((actions1, actions2), dim=0)\n",
    "        env_info = env.step([actions.cpu().numpy()])[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        states1 = states[0]\n",
    "        states2 = states[1]\n",
    "\n",
    "        if np.any(dones):\n",
    "            print('Episode {} finished. Scores reached: {}'.format(i_episode, scores))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ideas for future work\n",
    "\n",
    "\n",
    "To reach better results one can:\n",
    "\n",
    "- do extensive hyper parameter search e.g. using larger T_MAX_ROLLOUT, K_EPOCHS, and BATCH_SIZE\n",
    "- test the influence of the (USE_ENTROPY = True) term\n",
    "- use a different model architecture with more and deeper layers, e.g. even different networks for the actor and critic\n",
    "- run code on a GPU (e.g. with the headless version in the cloud)\n",
    "- utilize parallel training \n",
    "- more simulation time might yield better results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Credit\n",
    "\n",
    "This notebook is a project submission for the [Udacity Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). If you are interested to learn more about Deep Reinforcement Learning, I can highly recommend it! My contribution lays mainly in section 1, 5, 6, 7, and corresponding code files: `train.py`, `ppo_agent.py`, `policy.py` , and `watch_trained_agent.py`.\n",
    "\n",
    "Thanks and shoutout to [Jeremi Kaczmarczyk](https://github.com/jknthn), [Andrei Li](https://github.com/andreiliphd), and especially to [ShangtongZhang](https://github.com/ShangtongZhang). Parts of this code is based on their work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
